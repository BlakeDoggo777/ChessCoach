###############################################################################
#                           Network configuration                             #
###############################################################################

[network]

network_name = "selfplay11a"
role = "train|play"

[[networks]]

    name = "selfplay11a"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play" },

        # Train self-play, teacher
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },
        { stage = "save_swa", target = "teacher" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
    ]

    games_path_training = "Games/Fresh7a"

    [networks.self_play]

    network_type = "teacher"

[[networks]]

    name = "selfplay11"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play" },

        # Train self-play, teacher then distill
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },
        { stage = "save_swa", target = "teacher" },
        { stage = "train", target = "student" },
        { stage = "save", target = "student" },
        { stage = "save_swa", target = "student" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
        { stage = "strength_test", target = "student" },
    ]

    games_path_training = "Games/Fresh7"

[[networks]]

    name = "commentary1"
    
    [networks.training]

    stages = [
        # Train commentary, teacher
        { stage = "train_commentary", target = "teacher" },
        { stage = "save", target = "teacher" },
    ]

    steps = 400_000 # Equivalent to 50,000 steps of batch size 4096

    [networks.self_play]

    network_type = "teacher"

[[networks]]

    name = "supervised1"
    
    [networks.training]

    num_games = 2_000_000
    steps = 256_000

    value_loss_weight = 0.1
    mcts_value_loss_weight = 0.0

    stages = [
        # Train supervised, teacher
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
    ]

    games_path_training = "Games/Supervised"

[[networks]]

    name = "benchmark1"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play", window_size = 1_000_000, num_games = 1_440_000 },
    ]

    games_path_training = "Games/Benchmark"

    # NOTE: Still need to (a) turn off prediction cache if network untrained or testing without, (b) turn off uniform predictions if steps <= 0

###############################################################################
#     Default training and self-play configuration. Networks can override.    #
###############################################################################

[training]

num_games = 44_000_000
window_size = 1_000_000
batch_size = 512
commentary_batch_size = 512 # Use 64 on GTX 1080.
steps = 5_600_000 # Equivalent to 700,000 steps of batch size 4096
warmup_steps = 1000
pgn_interval = 10_000
validation_interval = 2000
checkpoint_interval = 10_000
strength_test_interval = 40_000
steps_per_execution = 50
value_loss_weight = 1.0
mcts_value_loss_weight = 0.15
policy_loss_weight = 1.0
momentum = 0.9
commentary_learning_rate_min = 1e-5 # Not multiplied by device count: range-test on each environment
commentary_learning_rate_max = 1e-3 # Not multiplied by device count: range-test on each environment
commentary_cyclic_step_size = 20_000 # Half of the cycle length
dataset_shuffle_positions_training = 524_288 # 2^19 (~9.5 GiB, 19568 bytes payload per position)
dataset_shuffle_positions_validation = 4096 # 2^12
dataset_keep_game_proportion = 0.2
dataset_keep_position_proportion = 0.1
dataset_parallel_reads = 32
swa_decay = 0.5 # Good in practice for 10k-checkpoints - adjust geometrically for different checkpoint sizes.
swa_minimum_contribution = 0.01 # Proportion, determines number of network checkpoints to average on resume.
swa_batchnorm_steps = 4000 # Becomes 500 actual steps on TPU. With default 0.99 batch normalization momentum, tested to be enough.
vocabulary_filename = "vocabulary.txt"
games_path_training = "Games/Training"
games_path_validation = "Games/Validation"
commentary_path = "Commentary"
wait_milliseconds = 300_000 # Check on Google Storage every 5 minutes when waiting for other machines.
stages = []

[training.learning_rate_schedule]

steps = [0, 800_000, 2_400_000, 4_000_000] # Equivalent to 100,000, 300,000, 500,000 with batch size 4096
rates = [2.0e-2, 2.5e-3, 2.5e-4, 2.5e-5] # Multiplied by device_count

[self_play]

network_type = "student"
# Instead of using the latest weights found for "network_name", use these specific ones; e.g., selfplay6a_001000000.
network_weights = ""

# Use 4*512 on GTX 1080.
num_workers = 8
prediction_batch_size = 512

num_sampling_moves = 30
max_moves = 512
num_simulations = 800

root_dirichlet_alpha = 0.3
root_exploration_fraction = 0.25

exploration_rate_base = 19652.0
exploration_rate_init = 1.25

linear_exploration_rate = 200.0
linear_exploration_base = 40000.0
virtual_loss_coefficient = 0.25
moving_average_build = 0.95
moving_average_cap = 1500.0
backpropagation_puct_threshold = 0.98
elimination_base_exponent = 8 # Start by giving the top 2^8 = 256 children linear exploration incentive, decaying down to 2.
move_diversity_value_delta_threshold = 0.01
move_diversity_temperature = 0.0
move_diversity_plies = 8

wait_for_updated_network = false

###############################################################################
#                       Miscellaneous configuration                           #
###############################################################################

[prediction_cache]

Hash = 8192 # Maps to PredictionCache_SizeMebibytes (named to auto-match UCI option).
max_ply = 30

[time_control]

safety_buffer_milliseconds = 100
fraction_remaining = 20
absolute_minimum_milliseconds = 50

[search]

# Use 4*256 on GTX 1080.
search_threads = 8
search_parallelism = 256
slowstart_nodes = 1024
slowstart_threads = 1
slowstart_parallelism = 32
gui_update_interval_nodes = 1000

[commentary]

top_p = 0.1
temperature = 1.5

[bot]

commentary_minimum_remaining_milliseconds = 30_000

[storage]

games_per_chunk = 2000

[paths]

# With the below config, a network may be saved to "gs://chesscoach-eu/ChessCoach/Networks/network_000001000".
tpu_data_root = "gs://chesscoach-eu/ChessCoach"

networks = "Networks"
tensorboard = "TensorBoard"
logs = "Logs"
pgns = "Pgns"
optimization = "Optimization"
alpha_manager = "AlphaManager"
syzygy = "Syzygy"

strength_test_marker_prefix = "StrengthTestComplete"

[optimization]

# Mode can be "epd" using "nodes required" metric or "tournament" using mini-tournament Elo metric.
mode = "epd"
resume_latest = false
log_interval = 10
plot_interval = 10

# "Nodes required" metric is the node count when the solution was first hit as the principle variation
# without later switching away, or "epd_failure_nodes" if the wrong answer was given for "bestmove".
# The first limit of "epd_movetime_milliseconds" and "epd_nodes" hit ends the search for each position,
# with 0 meaning no limit.
epd = "Arasan21.epd"
epd_movetime_milliseconds = 10_000
epd_nodes = 0
epd_failure_nodes = 10_000_000
epd_position_limit = 10

# Elo metric is calculated relative to baseline (Stockfish 13 NNUE with 1 thread and 512 MiB hash).
tournament_games = 10
tournament_time_control = "60+0.6" # 60+0.6 fast (1.8 min each for 80 moves), 600+6 medium (18 min each), 1200+12 slow (36 min each)

[optimization.parameters]

# Consumed by scikit-optimize, evaluated directly in Python. Examples:
# '(1, 5)'
# '(0.0, 1.0)'
# '(1e3, 1e6, "log-uniform")'
# '("small", "medium", "large")'
# NOTE: Updates are only seen by C++. Custom propagation is needed if Python needs to see an update.
# NOTE: Parameters must also be listed under [uci_options] when using mini-tournament-based optimization.
search_parallelism = '(1, 128)'
linear_exploration_rate = '(100.0, 500.0)'
linear_exploration_base = '(25000.0, 100000.0)'
virtual_loss_coefficient = '(0.1, 2.0)'
moving_average_build = '(0.5, 1.0)'
moving_average_cap = '(100.0, 50000.0)'
backpropagation_puct_threshold = '(0.0, 0.25)'

[uci_options]

# NOTE: Updates are only seen by C++. Custom propagation is needed if Python needs to see an update.
# NOTE: Some options can only be set before initialization; e.g., search threads and parallelism.
network_type = { type = "string" }
network_weights = { type = "string" }
search_threads = { type = "spin", min = 1, max = 256 }
search_parallelism = { type = "spin", min = 1, max = 4096 }
safety_buffer_milliseconds = { type = "spin", min = 0, max = 1000 }
Hash = { type = "spin", min = 0, max = 262_144 }
linear_exploration_rate = { type = "float" }
linear_exploration_base = { type = "float" }
virtual_loss_coefficient = { type = "float" }
moving_average_build = { type = "float" }
moving_average_cap = { type = "float" }
backpropagation_puct_threshold = { type = "float" }
syzygy = { type = "string" }

###############################################################################