###############################################################################
#                           Network configuration                             #
###############################################################################

[network]

network_name = "chesscoach1"
role = "train|play"

[[networks]]

    # Generate self-play games using the student network for speed until 800k steps.
    name = "selfplay11"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play" },

        # Train self-play, teacher then distill
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },
        { stage = "save_swa", target = "teacher" },
        { stage = "train", target = "student" },
        { stage = "save", target = "student" },
        { stage = "save_swa", target = "student" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
        { stage = "strength_test", target = "student" },
    ]

    games_path_training = "Games/Fresh7"

    steps = 800_000 # Equivalent to 100,000 steps of batch size 4096

[[networks]]

    # To prepare for teacher-only prediction, copy selfplay11_000800000 to selfplay11a_000800000
    # and copy 800k steps' worth of chunks from Fresh7 to Fresh7a (6286000 games, 3143 chunks).
    name = "selfplay11a"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play" },

        # Train self-play, teacher
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },
        { stage = "save_swa", target = "teacher" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
    ]

    games_path_training = "Games/Fresh7a"

    [networks.self_play]

    network_type = "teacher"

[[networks]]

    # To prepare for commentary training, copy selfplay11a_005600000 to selfplay11b_000000000,
    # delete teacher/model, and rename teacher/swa to teacher/model.
    name = "selfplay11b"
    
    [networks.training]

    stages = [
        # Train commentary, teacher
        { stage = "train_commentary", target = "teacher" },
        { stage = "save", target = "teacher" },
    ]

    steps = 400_000 # Equivalent to 50,000 steps of batch size 4096

    [networks.self_play]

    network_type = "teacher"

[[networks]]

    # To finalize training, copy selfplay11_003600000/student/swa, selfplay11a_005600000/teacher/swa and
    # selfplay11b_000400000/teacher/commentary into chesscoach1_005600000.
    name = "chesscoach1"

    [networks.training]

    stages = [
        # Already trained: just signal loading SWA weights.
        { stage = "save_swa", target = "teacher" },
        { stage = "save_swa", target = "student" },
    ]

    [networks.self_play]

    # Use "student" on GTX 1080 (applies to UCI; teacher is still required for commentary).
    network_type = "teacher"

[[networks]]

    name = "supervised1"
    
    [networks.training]

    num_games = 2_000_000
    steps = 256_000

    value_loss_weight = 0.1
    mcts_value_loss_weight = 0.0

    stages = [
        # Train supervised, teacher
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
    ]

    games_path_training = "Games/Supervised"

[[networks]]

    name = "benchmark1"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play", window_size = 1_000_000, num_games = 1_440_000 },
    ]

    games_path_training = "Games/Benchmark"

    # NOTE: Still need to (a) turn off prediction cache if network untrained or testing without, (b) turn off uniform predictions if steps <= 0

###############################################################################
#     Default training and self-play configuration. Networks can override.    #
###############################################################################

[training]

num_games = 44_000_000
window_size = 1_000_000
batch_size = 512
commentary_batch_size = 512 # Use 64 on GTX 1080, 128 on V100, 512 on v3-8 TPU.
steps = 5_600_000 # Equivalent to 700,000 steps of batch size 4096
warmup_steps = 1000
pgn_interval = 10_000
validation_interval = 2000
checkpoint_interval = 10_000
strength_test_interval = 40_000
steps_per_execution = 50
value_loss_weight = 1.0
mcts_value_loss_weight = 0.15
policy_loss_weight = 1.0
momentum = 0.9
commentary_learning_rate_min = 1e-5 # Not multiplied by device count: range-test on each environment
commentary_learning_rate_max = 1e-3 # Not multiplied by device count: range-test on each environment
commentary_cyclic_step_size = 20_000 # Half of the cycle length
dataset_shuffle_positions_training = 524_288 # 2^19 (~9.5 GiB, 19568 bytes payload per position)
dataset_shuffle_positions_validation = 4096 # 2^12
dataset_keep_game_proportion = 0.2
dataset_keep_position_proportion = 0.1
dataset_parallel_reads = 32
swa_decay = 0.5 # Good in practice for 10k-checkpoints - adjust geometrically for different checkpoint sizes.
swa_minimum_contribution = 0.01 # Proportion, determines number of network checkpoints to average on resume.
swa_batchnorm_steps = 4000 # Becomes 500 actual steps on TPU. With default 0.99 batch normalization momentum, tested to be enough.
vocabulary_filename = "vocabulary.txt"
games_path_training = "Games/Training"
games_path_validation = "Games/Validation"
commentary_path = "Commentary"
wait_milliseconds = 300_000 # Check on Google Storage every 5 minutes when waiting for other machines.
stages = []

[training.learning_rate_schedule]

steps = [0, 800_000, 2_400_000, 4_000_000] # Equivalent to 100,000, 300,000, 500,000 with batch size 4096
rates = [2.0e-2, 2.5e-3, 2.5e-4, 2.5e-5] # Multiplied by device_count

[self_play]

network_type = "student"
# Instead of using the latest weights found for "network_name", use these specific ones; e.g., selfplay6a_001000000.
network_weights = ""

# Use 4*512 on GTX 1080 (student), 4*512 on 4x V100 (student/teacher), 8*512 on v3-8 TPU (student/teacher).
num_workers = 8
prediction_batch_size = 512

num_sampling_moves = 30
max_moves = 512
num_simulations = 800

root_dirichlet_alpha = 0.3
root_exploration_fraction = 0.25

exploration_rate_base = 37_800.0
exploration_rate_init = 2.35
linear_exploration_rate = 4240.0
linear_exploration_base = 397_000.0
virtual_loss_coefficient = 0.05 # Mostly deferring to virtual exploration.
moving_average_build = 0.837
moving_average_cap = 8_000_000.0
backpropagation_puct_threshold = 0.912
elimination_base_exponent = 8 # Start by giving the top 2^8 = 256 children linear exploration incentive, decaying down to 2^1 = 2.
move_diversity_value_delta_threshold = 0.004
move_diversity_temperature = 4.0
move_diversity_plies = 12

wait_for_updated_network = false

###############################################################################
#                       Miscellaneous configuration                           #
###############################################################################

[prediction_cache]

Hash = 8192 # Maps to PredictionCache_SizeMebibytes (named to auto-match UCI option).
max_ply = 30

[time_control]

safety_buffer_milliseconds = 100
fraction_remaining = 20
absolute_minimum_milliseconds = 50

[search]

# Use 2*256 on GTX 1080 (student), 4*256 on 4x V100 (student/teacher), 8*256 on v3-8 TPU (student/teacher).
search_threads = 8
search_parallelism = 256
slowstart_nodes = 1024
slowstart_threads = 1
slowstart_parallelism = 32
gui_update_interval_nodes = 1000

[commentary]

top_p = 0.1
temperature = 1.5

[bot]

commentary_minimum_remaining_milliseconds = 30_000

[storage]

games_per_chunk = 2000

[paths]

# With the below config, a network may be saved to "gs://chesscoach-eu/ChessCoach/Networks/network_000001000".
# If the path can be accessed via tf.io.gfile then ChessCoach will run in a "cloud" configuration.
# This is currently only supported on Linux, as tf.io.gfile doesn't handle gs:// on Windows.
cloud_data_root = "gs://chesscoach-eu/ChessCoach"

networks = "Networks"
tensorboard = "TensorBoard"
logs = "Logs"
pgns = "Pgns"
optimization = "Optimization"
alpha_manager = "AlphaManager"
syzygy = "Syzygy"

strength_test_marker_prefix = "StrengthTestComplete"

[optimization]

# Mode can be "epd" using "nodes required" metric or "tournament" using mini-tournament Elo metric.
mode = "epd"
resume_latest = false
log_interval = 10
plot_interval = 10
distributed_zone = "europe-west4-a"
distributed_hosts = [] # Only supported with "tournament" mode and alpha TPU VMs/pods currently.
distributed_vs_stockfish = false

# "Nodes required" metric is the node count when the solution was first hit as the principal variation
# without later switching away, or "epd_failure_nodes" if the wrong answer was given for "bestmove".
# The first limit of "epd_movetime_milliseconds" and "epd_nodes" hit ends the search for each position,
# with 0 meaning no limit.
epd = "Arasan21.epd"
epd_movetime_milliseconds = 10_000
epd_nodes = 0
epd_failure_nodes = 10_000_000
epd_position_limit = 10

# Elo metric is calculated relative to baseline (Stockfish 13 NNUE with 1 thread and 512 MiB hash).
tournament_games = 10
tournament_time_control = "60+0.6" # 60+0.6 fast (1.8 min each for 80 moves), 300+3 slow (9 min each)

[optimization.parameters]

# Consumed by scikit-optimize, evaluated directly in Python. Examples:
# '(1, 5)'
# '(0.0, 1.0)'
# '(1e3, 1e6, "log-uniform")'
# '("small", "medium", "large")'
# NOTE: Updates are only seen by C++. Custom propagation is needed if Python needs to see an update.
# NOTE: Parameters must also be listed under [uci_options] when using mini-tournament-based optimization.
#exploration_rate_base = '(15_000.0, 50_000.0)'
#exploration_rate_init = '(1.6, 2.6)'
#linear_exploration_rate = '(2800.0, 5000.0)'
#linear_exploration_base = '(1e5, 1e6)'
#virtual_loss_coefficient = '(0.0, 0.5)'
#moving_average_build = '(0.7, 0.9)'
#moving_average_cap = '(1e6, 1e7)'
#backpropagation_puct_threshold = '(0.85, 0.95)'
#elimination_base_exponent = '(2, 10)'
move_diversity_value_delta_threshold = '(2e-3, 8e-3)'
move_diversity_temperature = '(0.25, 8.0)'
move_diversity_plies = '(4, 24)'

[uci_options]

# NOTE: Updates are only seen by C++. Custom propagation is needed if Python needs to see an update.
# NOTE: Some options can only be set before initialization; e.g., search threads and parallelism.
network_type = { type = "string" }
network_weights = { type = "string" }
search_threads = { type = "spin", min = 1, max = 256 }
search_parallelism = { type = "spin", min = 1, max = 4096 }
safety_buffer_milliseconds = { type = "spin", min = 0, max = 5000 }
Hash = { type = "spin", min = 0, max = 262_144 }
exploration_rate_base = { type = "float" }
exploration_rate_init = { type = "float" }
linear_exploration_rate = { type = "float" }
linear_exploration_base = { type = "float" }
virtual_loss_coefficient = { type = "float" }
moving_average_build = { type = "float" }
moving_average_cap = { type = "float" }
backpropagation_puct_threshold = { type = "float" }
elimination_base_exponent = { type = "spin", min = 2, max = 16 }
move_diversity_value_delta_threshold = { type = "float" }
move_diversity_temperature = { type = "float" }
move_diversity_plies = { type = "spin", min = 0, max = 512 }
syzygy = { type = "string" }

###############################################################################