###############################################################################
#                           Network configuration                             #
###############################################################################

[network]

network_name = "selfplay9"
role = "train|play"

[[networks]]

    name = "selfplay9a"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play" },

        # Train self-play, teacher
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },
        { stage = "save_swa", target = "teacher" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
    ]

    games_path_training = "Games/Fresh5"

    [networks.self_play]

    network_type = "teacher"

[[networks]]

    name = "selfplay9"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play" },

        # Train self-play, teacher then distill
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },
        { stage = "train", target = "student" },
        { stage = "save", target = "student" },
        { stage = "save_swa", target = "student" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
        { stage = "strength_test", target = "student" },
    ]

    games_path_training = "Games/Fresh5"

[[networks]]

    name = "chesscoach1"
    
    [networks.training]

    stages = [
        # Self-play
        { stage = "play" },

        # Train commentary and self-play, teacher
        { stage = "train_commentary", target = "teacher" },
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },

        # Distill self-play to student
        { stage = "train", target = "student" },
        { stage = "save", target = "student" },
        { stage = "save_swa", target = "student" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
        { stage = "strength_test", target = "student" },
    ]

[[networks]]

    name = "supervised1"
    
    [networks.training]

    num_games = 2_000_000
    steps = 256_000

    value_loss_weight = 0.1
    mcts_value_loss_weight = 0.015

    stages = [
        # Train supervised, teacher
        { stage = "train", target = "teacher" },
        { stage = "save", target = "teacher" },

        # Strength test (STS rating)
        { stage = "strength_test", target = "teacher" },
    ]

    games_path_training = "Games/Supervised"

[[networks]]

    name = "commentary1"
    
    [networks.training]

    stages = [
        # Train commentary, teacher
        { stage = "train_commentary", target = "teacher" },
        { stage = "save", target = "teacher" },
    ]

    [networks.training.commentary_learning_rate_schedule]

    steps = [0] #[0, 800_000, 2_400_000, 4_000_000]
    rates = [2e-2] #[2e-2, 2e-3, 2e-4, 2e-5]

[[networks]]

    name = "benchmark1"

    [networks.training]

    stages = [
        # Self-play
        { stage = "play", window_size = 1_000_000, num_games = 1_440_000 },
    ]

    games_path_training = "Games/Benchmark"

    # NOTE: Still need to (a) turn off prediction cache if network untrained or testing without, (b) turn off uniform predictions if steps <= 0

###############################################################################
#     Default training and self-play configuration. Networks can override.    #
###############################################################################

[training]

num_games = 44_000_000
window_size = 1_000_000
batch_size = 512
commentary_batch_size = 64
steps = 5_600_000 # Equivalent to 700,000 steps of batch size 4096
warmup_steps = 1000
pgn_interval = 10000
validation_interval = 2000
checkpoint_interval = 10000
strength_test_interval = 40000
steps_per_execution = 50
value_loss_weight = 1.0
mcts_value_loss_weight = 0.15
policy_loss_weight = 1.0
momentum = 0.9
dataset_shuffle_positions_training = 524_288 # 2^19 (~9.5 GiB, 19504 bytes payload per position)
dataset_shuffle_positions_validation = 4096 # 2^12
dataset_keep_game_proportion = 0.2
dataset_keep_position_proportion = 0.1
dataset_parallel_reads = 32
swa_decay = 0.5 # Good in practice for 10k-checkpoints - adjust geometrically for different checkpoint sizes.
swa_minimum_contribution = 0.01 # Proportion, determines number of network checkpoints to average on resume.
swa_batchnorm_steps = 4000 # Becomes 500 actual steps on TPU. With default 0.99 batch normalization momentum, tested to be enough.
vocabulary_filename = "vocabulary.txt"
games_path_training = "Games/Training"
games_path_validation = "Games/Validation"
commentary_path_training = "Commentary/Supervised"
commentary_path_validation = ""
wait_milliseconds = 300_000 # Check on Google Storage every 5 minutes when waiting for other machines.
stages = []

[training.learning_rate_schedule]

steps = [0, 800_000, 2_400_000, 4_000_000] # Equivalent to 100,000, 300,000, 500,000 with batch size 4096
rates = [2.5e-2, 2.5e-3, 2.5e-4, 2.5e-5] # Multiplied by device_count

[training.commentary_learning_rate_schedule]

steps = [0] #[0, 800_000, 2_400_000, 4_000_000] # Equivalent to 100,000, 300,000, 500,000 with batch size 4096
rates = [2e-4] #[2e-4, 2e-5, 2e-6, 2e-7] # Multiplied by device_count

[self_play]

network_type = "student"
# Instead of using the latest weights found for "network_name", use these specific ones; e.g., selfplay6a_001000000.
network_weights = ""

num_workers = 8
prediction_batch_size = 512

num_sampling_moves = 30
max_moves = 512
num_simulations = 800
deep_simulations_count = 10_000 # (10k/0.05) combination increases self-play work by 57.5%.
deep_simulations_proportion = 0.05
syzygy_probe_proportion = 0.15 # Used for both root WDL+DTZ and leaf WDL probes.

root_dirichlet_alpha = 0.3
root_exploration_fraction = 0.25

exploration_rate_base = 19652.0
exploration_rate_init = 1.25

linear_exploration_rate = 200.0
linear_exploration_base = 10000.0 # Should roughly equal "deep_simulations_count" for reasonable exploration and policy target shape.
virtual_loss_coefficient = 0.25
moving_average_build = 0.95
moving_average_cap = 1500.0
backpropagation_puct_threshold = 0.98
elimination_base_exponent = 8 # Start by giving the top 2^8 = 256 children linear exploration incentive, decaying down to 2.
move_diversity_value_delta_threshold = 0.01
move_diversity_temperature = 0.0

wait_for_updated_network = false

###############################################################################
#                       Miscellaneous configuration                           #
###############################################################################

[prediction_cache]

Hash = 8192 # Maps to PredictionCache_SizeMebibytes (named to auto-match UCI option).
max_ply = 30

[time_control]

safety_buffer_milliseconds = 100
fraction_remaining = 20

[search]

search_threads = 4
search_parallelism = 256
gui_update_interval_nodes = 1000

[storage]

games_per_chunk = 2000

[paths]

# With the below config, a network may be saved to "gs://chesscoach-eu/ChessCoach/Networks/network_000001000".
tpu_data_root = "gs://chesscoach-eu/ChessCoach"

networks = "Networks"
tensorboard = "TensorBoard"
logs = "Logs"
pgns = "Pgns"
optimization = "Optimization"
alpha_manager = "AlphaManager"
syzygy = "Syzygy"

strength_test_marker_prefix = "StrengthTestComplete"

[optimization]

# Mode can be "epd" using "nodes required" metric or "tournament" using mini-tournament Elo metric.
mode = "epd"
resume_latest = false
log_interval = 10
plot_interval = 10

# "Nodes required" metric is the node count when the solution was first hit as the principle variation
# without later switching away, or "epd_failure_nodes" if the wrong answer was given for "bestmove".
# The first limit of "epd_movetime_milliseconds" and "epd_nodes" hit ends the search for each position,
# with 0 meaning no limit.
epd = "Arasan21.epd"
epd_movetime_milliseconds = 10_000
epd_nodes = 0
epd_failure_nodes = 10_000_000
epd_position_limit = 10

# Elo metric is calculated relative to baseline (Stockfish 13 NNUE with 1 thread and 512 MiB hash).
tournament_games = 10
tournament_movetime_milliseconds = 1000

[optimization.parameters]

# Consumed by scikit-optimize, evaluated directly in Python. Examples:
# '(1, 5)'
# '(0.0, 1.0)'
# '(1e3, 1e6, "log-uniform")'
# '("small", "medium", "large")'
# NOTE: Updates are only seen by C++. Custom propagation is needed if Python needs to see an update.
# NOTE: Parameters must also be listed under [uci_options] when using mini-tournament-based optimization.
search_parallelism = '(1, 128)'
linear_exploration_rate = '(100.0, 500.0)'
linear_exploration_base = '(25000.0, 100000.0)'
virtual_loss_coefficient = '(0.1, 2.0)'
moving_average_build = '(0.5, 1.0)'
moving_average_cap = '(100.0, 50000.0)'
backpropagation_puct_threshold = '(0.0, 0.25)'

[uci_options]

# NOTE: Updates are only seen by C++. Custom propagation is needed if Python needs to see an update.
# NOTE: Some options can only be set before initialization; e.g., search threads and parallelism.
network_type = "string"
network_weights = "string"
search_threads = "spin"
search_parallelism = "spin"
safety_buffer_milliseconds = "spin"
Hash = "spin"
linear_exploration_rate = "float"
linear_exploration_base = "float"
virtual_loss_coefficient = "float"
moving_average_build = "float"
moving_average_cap = "float"
backpropagation_puct_threshold = "float"
syzygy = "string"

###############################################################################