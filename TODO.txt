Next
- uci
	- crash in UCI game when reusing position + parallelism? debug latest uci log
	- debug mate-in-one with MCTS parallelism: 6B1/k7/3pP2P/5p2/p2R4/P1N5/8/1RK2N2 w - - 0 47
	- work on faster/forced mates
	- write out pv every N seconds if no updates
	- write out best move (@ arena)
	- make training network loading lazy for faster uci?
	- try to increase in-thread parallelism without sacrificing STS strength
		- some kind of idea around only the first N backpropagating, the rest just setting up children/priors?
		- can always look into some of the failing positions and see what's going differently (harder without multiPV though)
	- also test different FPU schemes for strength, but care, and def. don't change training
	- try multi-thread
		- need to sync std::cout - use Reply() everywhere
- unify config across C++/Python
	- match run name to network folder names, selectively load
- implement endgame tablebases (for both UCI and self-play, but don't cut game short, let NN learn)
	- see if we can reuse stockfish probing
	- actually we need it to make mistakes in the endgame to learn, best play isn't enough - need noise or temperature
- add "other" param to storage tests, make sure games don't bleed between types
- fixes
	- crash starting selfplay right after strength tests?
	- maybe prune old game in SetUpGame, look into where it should be happening, assert somewhere for real games
- any more cache tuning? - hits about 70% full 40% hit rate after 3142 games, 12 ply max
	- while not full hit rate should be higher? add some new metrics

Self-play performance
- try shrink Node memory
- try different NN architecture like attention/transformer
- try distilling to a smaller network for self-play predictions

Value head convergence:
- try AZ history planes instead of mine
	- try more lookup tables for plane mappings, e.g. byte/short slices of float rows

Network architecture/training data
- try training value against lerped Z/Q a la Oracle blog
- try v2 residuals
- try some additional curriculum heads; e.g. autoencode position, or count piece types, or linear material imbalance
- try Wu improvements (accelerating go learning)
- try deformable CNNs
- try 32-filter value convolution once enough data to compare
	- although, this may reduce reuse/regularization between value/policy - do it way later when it can play chess
- experiment with full curriculum learning, endgames backwards, slowly add piece types
- revisit full Oracle blog series
- @gcp saying MSE overfitting inevitable without 100ks of games; maybe look into pseudonegatives soon
	- is it possible to filter them through an MCTS or something to ensure the data are still "chess"-y?
- we could estimate duplication factors for positions in early plies and de-weight them in the sampling distribution slightly?
- can always retry SE in future when solid architecture/visualization
- turn down the stockfish eval blend factor as training progresses

Performance
- break down MCTS timing details just in case anything's off (just use sampling)
- try use inline map in Node rather than heap-allocated (limit to 80? 480 extra bytes plus overhead)
	- maybe just linear lookup, no log-time involved? investigate by-key vs. iteration cases
	- can always binary search Move-ints or something too, it's not mutated
- check inlining once CPU tree is looking good
- try Intel compiler in future
- image key for prediction cache can be incrementally updated if it helps enough
- do some timeline charting for CPU vs. GPU use
	- it may make sense to keep shared images/values/policies but have separate CPU worker threads
	  drop in/pick up there, using a GPU worker like very beginning, and maybe oversubscribe
	  like double-buffer games? might just be equivalent to doubling threads though. seem to be
	  limited by Node memory use now. anyway, think about it.

Code quality
- rename NN "test" to "validate" to avoid confusion with unit tests and keep consistent with tensorboard
- privatize C++ methods (got too python-brained)
- try split up MCTS vs. selfplaygames better - e.g. maybe move softmax/selectmove out? separate files?
- try split SelfPlayGame/Worker vs. "UciGame/Worker" better somehow (including image/value/policy arrays, batch sizes, etc.)
- add Python type info everywhere - look up best practices for linting etc.

GUI for debugging
- develop/use low-cost GUI for debugging MCTS, value, policy
- look into helping-to-get-mated
- also try better text printouts, e.g. at X simulations print Y% win probability and PV

UCI
- implement worthwhile options (work out hash, TT vs. prediction cache)
- initial tournament really shows we need mate proving or more nodes/forced breadth, keeps getting mated in one
  even when winning vs. other engines
- workaround for .exe vs. non in ChessBase, test as kibitzer
- implement proper time controls/search options, ponder
- "random move with eval within 1%"
	- http://blog.lczero.org/2018/12/alphazero-paper-and-lc0-v0191.html?m=1
	- https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd
- implement multiPV (run/batch multiple games in a worker with path restrictions?)

Scalability
- can't play enough self-play games, work on new ideas soon
- can't afford 1 million games in the window (RAM or disk), work something out

Deployment
- add linux support
- installers, instructions, progress indicators, error handling, etc.

Low priority
- see if it's more efficient saving/loading checkpoints (measure save/load time, prediction time, disk savings)
- 80 cap on branched moves w/ cache may be a problem later; applies before d-noise, so limits exploration
- structured binding, check all pairs
- make sure killing uci.cmd kills exe (it's complicated - killing cmd.exe doesn't, but closing its window does - need to test programmatically)

Revisit
- try batch renorm + ghost batch norm
	- Bug: https://github.com/tensorflow/tensorflow/issues/32380