TODO
- check 2f training run vs. 2d
- add SE to whichever is better (still with commentary)
- evaluate, check in L2 and/or SE

TODO
- try to make mate-visiting stop wasting time, instantly push up to 2nd best
	- complicated with multiple levels of visits
	- complicated if 2nd best is also mate/terminal
- need to fix 350 NPS situation in stockfish games
	- try out a backtracking implementation of virtual exploration/loss that goes as deep
	  as it can into good lines and can choose between 2nd best here vs. 2nd best up one level, etc.,
	  choosing better nodes overall and avoiding failed simulations

Training improvements
- SE (lc0)
	- try batchnorm/relu before SE since we have v2 residuals
- SE-ish-as-bias (Wu)
	- (TODO)
- playout cap randomization (Wu)
- pseudonegatives (look up the paper again, may be difficult/impossible for chess, invalid positions)
- try 1 vs. 8 vs. 32 value head filters again
	- lc0/Wu seem to use similar conv filters for policy and value heads?
	- lc0 was thinking fully connected layer may be needed for policy head when network isn't deep enough to cross board well (may not apply for distillation)
- try a run with masked policy loss (valid moves only) once there's enough data to compare
	- could also conceivably blend, like 1.0 for legal moves and 0.15 zero-pressure for others
- find some positions where engine misses shallow tactics, play with FPU and criticality
	- try to quantify and train criticality, feed into FPU (or deeper FPU, multi-move decay)
	- training target needs to be good/consistent even when already good at predicting; e.g. can't base on how wrong we were before searching, oscillate - intrinsic to position
- sketch out "value trust curve", especially with volatile game, multiple mistakes both sides
	- work out static eval vs. search on stockfish side for inflection/control points, see what's viable
- investigate FROST - need to read more, but applied to data efficiency, seems like we could go in a couple directions:
	- use as intended, grab a bunch of unlabeled games, gradually pseudo-label better (end up with more data)
	- treat existing labels as pseudo-labels (e.g. poor-confidence value-from-final-result) and refine (end up with improved label quality)

GUI
- "gui moves f3e5", like ~ ucb, pin data pushes to that sub-position and show history over it?
- deal with info panel height/scrolling, move list
- change the square background rather than adding overlay, need more consistency between black/white squares
- implement search tool with slider over node count
- websocket reconnect

Cluster
- still waiting on core dump issue

Bigger ideas
- what if we just shove the supervised games at the beginning of a larger "virtual" window, so that training starts with 100% supervised and finishes with 0%
	- window would need to start large, e.g. 1m or 2m, then shrink fast enough to reach milestones like 50% selfplay by X steps, 0% by Y steps
	  (without requiring many millions of self-play games)
- seems like networks may be getting too "set in their ways" with early unfortunate/shortcut data and heavy sample ratio
	- could use a generation-style system to periodically start new networks fresher with less supervised/early and lower sample ratio,
	  now that they have more data available to them, then take over on self-play once they emperically reach parity

Speed up training, play with tf.function
	- don't log step 1 to tensorboard (switch to just before, not just-after? in conjunction with multi-step-in-tf-function)
	- TPU training perf:
	- https://github.com/tensorflow/tensorflow/issues/20068
	- https://github.com/tensorflow/tpu/tree/master/models/official/resnet

Quality
- modify config.toml directly in sub-dockerfiles, don't rebuild/reinstall
- print config summary when starting a run
- switch to even number of history positions for repetitions (what if we only did every 2nd going back)
	- wait for a solid baseline and measure

Commentary cleanup
- use model garden transformer now?
- try wrap main engine loop with @tf.function too?
- cull attention weight plumbing
- try time and special-case speed-up prediction for batchsize=1

Text quality
- try bigger model, get some convergence somehow (get 25% with fixed batch at least, work up from there)
- basic - does the encoder need an extra dense+relu layer after the tower/reshape?
- larger model size
- different model architectures
	- think about 1D vs. 2D again: https://arxiv.org/pdf/1502.03044.pdf
	- read about "Performer" architecture
	- read about PET: https://arxiv.org/pdf/2001.07676.pdf
	- definitely try universal transformer
- different tokenizers
	- need TF.Text for Windows in TF2.4 release so we can sub-word tokenize in graph mode (SubwordTextEncoder doesn't work in graph, deprecated)
	- lots of options for chess specifically, around punctuation, top-k, etc.
- build deterministic train/validate split?
- try encode before + after positions - comment on the move, not the position
	- yes, image takes last N positions, but network is trained to evaluate the position, not the delta

Additional head ideas:
- (Wu): "whenever a desired target can be expressed as a sum, conjunction, or disjunction of separate subevents, or would be highly correlated with such subevents, predicting those subevents is likely to help""
- autoencode position
- count all pieces?
- count total square control/ratio/difference/"policy"?
- count piece types
- count piece imbalances
- predict opponent move (Wu uses 0.15 factor)
- principle variation (big output surface, complicated idea)
- predict final board state or piece distribution (like Wu's final ownership)
- king safety, mobility, etc.

At 20-30k steps:
- test without mcts value/reply policy, see whether they truly help/hurt
- try out "sleeping" in the rotation - train random noise, different distributions
- make sure piece-value understanding is improving; think about standard A/B positions to log avg queen/rook/etc. value to tensorboard

Once value loss is converging below 0.4 on big network:
- Try knowledge distillation for regression ideas: http://www.cs.ox.ac.uk/files/11078/ICCV19_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network.pdf

Training - later
- once we're getting to 2600-3200, test raw student-as-teacher training to make sure distillation is worth it
- like Wu/lc0/SAI, think about finish-fast utility score added to win score - way later though, only useful if it plays well but dithers
- stochastic weight averaging

TensorFlow training speed (mostly dataset issues, local/GPU speed regression)
- move to ~100MB-sized chunks as recommended (but make sure self-play can still viably train)
- try also batching across examples in dataset.py
- need to solve pipeline threading issues again, make sure Cuda >= 90% locally
	- also "experimental_steps_per_execution" for TPU
- throw away positions before expensive decompression in dataset.py, keep all/more games, keep fewer positions
- run full local training to validate

UCI (#2)
- tune on GPU and TPU (methods below)
- try to increase in-thread parallelism without sacrificing STS strength
	- some kind of idea around only the first N backpropagating, the rest just setting up children/priors?
	- can always look into some of the failing positions and see what's going differently (harder without multiPV though)
- also test different FPU schemes for strength, but care, and def. don't change training
- try multi-thread
	- need to sync std::cout - use Reply() everywhere
- basic improvements to time control
	- don't waste time when only 1 or few legal moves
	- if dangerous time left, take mate-in-N rather than trying to find faster

CPU+GPU integration performance:
- try vectorize/GPU softmax/UCT calcs
- break down MCTS timing details just in case anything's off (instrumented, already did some sampling measurements/optimization)
- image key for prediction cache can be incrementally updated if it helps enough
- do some timeline charting for CPU vs. GPU use
	- it may make sense to keep shared images/values/policies but have separate CPU worker threads
	  drop in/pick up there, using a GPU worker like very beginning, and maybe oversubscribe
	  like double-buffer games? might just be equivalent to doubling threads though. seem to be
	  limited by Node memory use now. anyway, think about it.
- for Nodes can point last sibling at root and unrecurse deletion (sacrificing breadth-at-once)

Code quality
- try multistage docker build, debian only for building chesscoach, copy over to tensorflow base
- inject config.toml into cluster volume, take out of image tags, make sure it takes priority over installed
- copy config.toml from install->user on run (if not present) - optimized for dev iteration atm
- switch to vcpkg for windows dependencies
- fix _ in network names - split on final _
- factor out all-exist ringbuffer with foreach iteration (begin/end)
- privatize C++ methods (got too python-brained)
- try split up MCTS vs. selfplaygames better - e.g. maybe move softmax/selectmove out? separate files?
- try split SelfPlayGame/Worker vs. "UciGame/Worker" better somehow (including image/value/policy arrays, batch sizes, etc.)
- improve config use in C++, feels messy
- add Python type info everywhere - look up best practices for linting etc.
- poison self-play workers and join threads properly
- check for any unnecessary move constructors
- add further architecture configuration to toml config (residuals*filters, etc.)
- switch to sample ratio, rather than specifying games + steps (complicated, ties in to "curriculum schedule" in config maybe)
- StrengthTestNetwork (naming)

UCI (#3)
- "random move with eval within 1%"
	- if best is value N, softmax sample with temperature 10 among (value >= N - 0.01), where 0.01 = (win-loss)/100
	- "best" I'm assuming is the otherwise usual selection, i.e. most visited
	- code for shorter mates, etc., should override this behavior (no diversity needed then)
- mate
	- if root node's terminal value is known, and allowed to stop searching, make a move instantly and save time
	- add global depth cut-off (a little tricky, need to allow MCTS to "back up" and ban paths that don't reach a leaf in time)
	- add killer heuristic (needs to vanish quickly with exploration)
		- test with M1 position with M1 prior artifically set low for ply 0
		- also strength-test
	- forced draw propagation if it'll help
- look into LCB move selection (Wu, LZ)
- implement worthwhile options (work out hash, TT vs. prediction cache)
- workaround for .exe vs. non in ChessBase, test as kibitzer
- implement proper time controls/search options, ponder

Deployment
- installers, instructions, progress indicators, error handling, etc.
- linux unit test script

Publishing
- write-ups
- website - game-in-10, one at a time, people can watch, comments shown for each move

Revisit
- try batch renorm + ghost batch norm
	- Bug: https://github.com/tensorflow/tensorflow/issues/32380
- any more cache tuning? - hits about 70% full 40% hit rate after 3142 games, 12 ply max
	- while not full hit rate should be higher? add some new metrics

Bugs
- Read-AV in game stateinfo cleanup when quitting UCI
- Some kind of race condition spamming next-chunk button in GUI, locks up Python/C++ interop
- Close cleanly if selfplay/UCI network not found by name in config list, in Python or C++
- SelfPlayWorker objects are created on the main thread and use its StateInfo in Game::Game(), then the worker thread steals in Free
- Still may be a leak between multiple self-play rotations separated by training - doesn't require strength testing to repro
	- Looks like it's actually the training code - need to investigate in Python
- Tiny configs with self-play don't work now with minimum 2000 chunking
- network.js doesn't listen to CHESSCOACH_SILENT when set in NetworkTest.cpp

Low priority
- shouldn't use prediction cache with uniform predictions (CPU) - ugly to plumb
- anything that uses the mainWorker will create new poolallocators on the primary (non-worker) thread, higher memory ceiling
	- unavoidable if we keep all games around to resume, need higher ceiling
- save fewer networks to cloud storage or cull old ones in between strength tests or something - 100 MB per save
- structured binding, check all pairs

Won't do
- TensorRT or TF C++ API (overly complicated setup, especiallly building TF on Windows, embarrassingly bad)
- Richer UCI features like multiPV
- CPU optimization - not measurable enough while GPU-dominated
- endgame tablebases (e.g. leveraging Stockfish probing code)
	- UCI: not interesting enough, others can add if going for pure strength
	- Training: complicated, scope too large - self-play needs to make mistakes to learn imbalances