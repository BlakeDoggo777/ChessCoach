Next
- once we're graphing strength, run more long supervised runs with e.g. 32-value head, different loss ratios, etc.
	- see if we can get the value head to converge? still really high, policy compensates in games though
	- try smaller batch size until ghost batches works or I reimplement
	- try AZ history planes instead of mine
- self-play
	- work out multi-game writing to disk (maybe just open rw, update count, append?)
- add tests for FlipMove, etc.
- uci
	- don't throw away trees if new position is just a child? log received commands to file to debug
	- work out what lc0 reports for NPS, numbers are way off
	- multithreading, virtual loss, any in-thread multiplexing?
- prediction cache
	- clean up prediction cache clear printing
	- more LRU
		- try linear probing or small buckets (rename current buckets to tables)
	- use during UCI (options?)
- unify config across C++/Python
- initial tournament really shows we need mate proving or more nodes/forced breadth, keeps getting mated in one
  even when winning vs. other engines

Network architecture/training data
- try training value against lerped Z/Q a la Oracle blog
- try v2 residuals
- try some additional curriculum heads; e.g. autoencode position, or count piece types, or linear material imbalance
- try Wu improvements (accelerating go learning)
- try deformable CNNs
- try 32-filter value convolution once enough data to compare
	- although, this may reduce reuse/regularization between value/policy - do it way later when it can play chess
- experiment with endgame tablebases integrated
- experiment with full curriculum learning, endgames backwards, slowly add piece types
- revisit full Oracle blog series
- @gcp saying MSE overfitting inevitable without 100ks of games; maybe look into pseudonegatives soon
	- is it possible to filter them through an MCTS or something to ensure the data are still "chess"-y?
- we could estimate duplication factors for positions in early plies and de-weight them in the sampling distribution slightly?
- can always retry SE in future when solid architecture/visualization
- turn down the stockfish eval blend factor as training progresses

Performance
- try more lookup tables for plane mappings, e.g. byte/short slices of float rows
- break down MCTS timing details just in case anything's off (just use sampling)
- try use inline map in Node rather than heap-allocated (limit to 80? 480 extra bytes plus overhead)
	- maybe just linear lookup, no log-time involved? investigate by-key vs. iteration cases
	- can always binary search Move-ints or something too, it's not mutated
- check inlining once CPU tree is looking good
- try Intel compiler in future

Code quality
- rename NN "test" to "validate" to avoid confusion with unit tests and keep consistent with tensorboard
- privatize C++ methods (got too python-brained)
- try split up MCTS vs. selfplaygames better - e.g. maybe move softmax/selectmove out? separate files?
- try split SelfPlayGame/Worker vs. "UciGame/Worker" better somehow (including image/value/policy arrays, batch sizes, etc.)
- add Python type info everywhere - look up best practices for linting etc.

GUI for debugging
- develop/use low-cost GUI for debugging MCTS, value, policy
- look into helping-to-get-mated
- also try better text printouts, e.g. at X simulations print Y% win probability and PV

UCI
- workaround for .exe vs. non in ChessBase, test as kibitzer
- implement proper time controls/search options, ponder
- multi-thread MCTS
	- "On the scalability of parallel UCT"
	- https://medium.com/oracledevs/lessons-from-alpha-zero-part-5-performance-optimization-664b38dc509e
	- need to sync std::cout
- "random move with eval within 1%"
	- http://blog.lczero.org/2018/12/alphazero-paper-and-lc0-v0191.html?m=1
	- https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd
- implement multiPV (run/batch multiple games in a worker with path restrictions?)
- implement endgame tablebases (invisible/external to the training loop?)

Scalability
- can't play enough self-play games, work on new ideas soon
- can't afford 1 million games in the window (RAM or disk), work something out

Deployment
- add linux support
- installers, instructions, progress indicators, error handling, etc.

Low priority
- see if it's more efficient saving/loading checkpoints (measure save/load time, prediction time, disk savings)
- 80 cap on branched moves w/ cache may be a problem later; applies before d-noise, so limits exploration
- structured binding, check all pairs
- implement write-to-pgn in C++, stop depending on python-chess (just stockfish)

Revisit
- try batch renorm + ghost batch norm
	- Bug: https://github.com/tensorflow/tensorflow/issues/32380