Attention network experiments:
- try eliminate initial convolution
- try 32-filter value head
- try 32-filter attention value head
- try v2 residuals
- try different learning rates
- try different policy/value learning weights
- try longer tower
- try longer stem
- try different heads

Attention:
- add architecture configuration to toml config (type, residuals*filters, etc.)
- really need value to converge

Final architecture vetting before self-play perf focus (ensure value head convergence, etc.)
- try to train on some lc0 games or something to make sure everything's working/bug-free
- pipeline batch sampling in C++, too slow

Self-play performance
- try shrink Node memory
- try distilling to a smaller network for self-play predictions

Improve convergence speed
- do a big write-up of Oracle/Wu/own ideas and choose top N
- try different NN architecture like attention/transformer (focus on strength)
- try training value against lerped Z/Q a la Oracle blog
- try v2 residuals
- try some additional curriculum heads; e.g. autoencode position, or count piece types, or linear material imbalance
- try Wu improvements (accelerating go learning)
- try deformable CNNs
- try 32-filter value convolution once enough data to compare
	- although, this may reduce reuse/regularization between value/policy - do it way later when it can play chess
- experiment with full curriculum learning, endgames backwards, slowly add piece types
- revisit full Oracle blog series
- @gcp saying MSE overfitting inevitable without 100ks of games; maybe look into pseudonegatives soon
	- is it possible to filter them through an MCTS or something to ensure the data are still "chess"-y?
- we could estimate duplication factors for positions in early plies and de-weight them in the sampling distribution slightly?
- can always retry SE in future when solid architecture/visualization
- turn down the stockfish eval blend factor as training progresses

UCI (#2)
- try to increase in-thread parallelism without sacrificing STS strength
	- some kind of idea around only the first N backpropagating, the rest just setting up children/priors?
	- can always look into some of the failing positions and see what's going differently (harder without multiPV though)
- also test different FPU schemes for strength, but care, and def. don't change training
- try multi-thread
	- need to sync std::cout - use Reply() everywhere
- basic improvements to time control
	- don't waste time when only 1 or few legal moves
	- if dangerous time left, take mate-in-N rather than trying to find faster

CPU/integration performance:
- break down MCTS timing details just in case anything's off (just use sampling)
- try more lookup tables for plane mappings, e.g. byte/short slices of float rows
- try use inline map in Node rather than heap-allocated (limit to 80? 480 extra bytes plus overhead)
	- maybe just linear lookup, no log-time involved? investigate by-key vs. iteration cases
	- can always binary search Move-ints or something too, it's not mutated
- check inlining once CPU tree is looking good
- try Intel compiler in future
- image key for prediction cache can be incrementally updated if it helps enough
- do some timeline charting for CPU vs. GPU use
	- it may make sense to keep shared images/values/policies but have separate CPU worker threads
	  drop in/pick up there, using a GPU worker like very beginning, and maybe oversubscribe
	  like double-buffer games? might just be equivalent to doubling threads though. seem to be
	  limited by Node memory use now. anyway, think about it.

Code quality
- copy config.toml from install->user on run (if not present) - optimized for dev iteration atm
- fix _ in network names - split on final _
- factor out all-exist ringbuffer with foreach iteration (begin/end)
- privatize C++ methods (got too python-brained)
- try split up MCTS vs. selfplaygames better - e.g. maybe move softmax/selectmove out? separate files?
- try split SelfPlayGame/Worker vs. "UciGame/Worker" better somehow (including image/value/policy arrays, batch sizes, etc.)
- improve config use in C++, feels messy
- add Python type info everywhere - look up best practices for linting etc.
- poison self-play workers and join threads properly

GUI for debugging
- develop/use low-cost GUI for debugging MCTS, value, policy
- look into helping-to-get-mated
- also try better text printouts, e.g. at X simulations print Y% win probability and PV

UCI (#3)
- "random move with eval within 1%"
	- if best is value N, softmax sample with temperature 10 among (value >= N - 0.01), where 0.01 = (win-loss)/100
	- "best" I'm assuming is the otherwise usual selection, i.e. most visited
	- code for shorter mates, etc., should override this behavior (no diversity needed then)
- mate
	- if root node's terminal value is known, and allowed to stop searching, make a move instantly and save time
	- add global depth cut-off (a little tricky, need to allow MCTS to "back up" and ban paths that don't reach a leaf in time)
	- add killer heuristic (needs to vanish quickly with exploration)
		- test with M1 position with M1 prior artifically set low for ply 0
		- also strength-test
	- forced draw propagation if it'll help
- implement endgame tablebases (for UCI; self-play could be dangerous, needs more thought)
	- see if we can reuse stockfish probing
	- for self-play, definitely don't cut games short, play it out
	- need it to make mistakes in the endgame to learn, best play isn't enough - need noise or temperature
- implement worthwhile options (work out hash, TT vs. prediction cache)
- workaround for .exe vs. non in ChessBase, test as kibitzer
- implement proper time controls/search options, ponder
- write out currmove et al (e.g. for Arena)
- implement multiPV (run/batch multiple games in a worker with path restrictions?)

TPUs (if needed)
- get it running on cloud
- scale up training + inference
- try mixed mode (16/32)

Scalability
- can't play enough self-play games, work on new ideas soon
- can't afford 1 million games in the window (RAM or disk), work something out

Deployment
- installers, instructions, progress indicators, error handling, etc.

Low priority
- see if it's more efficient saving/loading checkpoints (measure save/load time, prediction time, disk savings)
- 80 cap on branched moves w/ cache may be a problem later; applies before d-noise, so limits exploration
- structured binding, check all pairs
- make sure killing uci.cmd kills exe (it's complicated - killing cmd.exe doesn't, but closing its window does - need to test programmatically)

Revisit
- try batch renorm + ghost batch norm
	- Bug: https://github.com/tensorflow/tensorflow/issues/32380
- any more cache tuning? - hits about 70% full 40% hit rate after 3142 games, 12 ply max
	- while not full hit rate should be higher? add some new metrics