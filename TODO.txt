Next
- running (6g): extension of existing full run with teacher-only prediction (w/ SWA, conditional config if it works)
- training run with same-perspective history planes (need a way to swap bytes in graph mode - run all network tests)

UCI/tournaments/parallelism
- try out teacher prediction in Arasan21, tournaments, etc. - adjust parallelism

Training - fresh run
	- recent PUCT changes (draw-sibling-FPU, etc.)
	- win-FPU at root
	- (maybe) same-perspective history planes
	- selfplay7, Fresh3

Training - after PUCT work
- try full run with 1/N positions using K nodes (e.g. 1/20, 50k) - acts like auxiliary head at low loss weight, points good direction
	- need to emulate parallelism/vloss/wideband for the 50k?
	- selfplay8, Fresh4

Commentary #3
- need fix for tensorflow_text on alpha TPU VMs - lazy-import for now
- remove dropout for now - can slowly add back decoder-only - may need to avoid it for highly discrete board squares in the encoder sequence
- investigate inference problems locally (initial code is stashed)
- build small manual test set for commentary inference
- may need to increase maximum sequence length, with sub-word tokenization
- maybe backprop into encoder (sometimes? always?)
- compare interleaving with main model training vs. "at the end"
- compare adam and sgd-momentum for commentary
	- try varying warmup steps
	- try flip to pre-LN, get rid of warmup everywhere: https://openreview.net/pdf?id=B1x8anVFPr
- review/tune commentary quality
- compare beam and top-k
	- look back through my notes, was pretty pessimistic about beam based on articles a few months back

Commentary #4
- try pre or post-processing with GPT-2
- try data augmentation using BERT: https://www.researchgate.net/publication/343958685_Text_Augmentation_Using_BERT_for_Image_Captioning
- maybe shrink model if still not enough data

Commentary #5
- for interleaving regular/commentary training - read up on "learning without forgetting"
- basic - does the encoder need an extra dense+relu layer after the tower/reshape?
	- basically 256->512 embedding rather than categorical->512 embedding
- try encode before-and-after positions (no flip?) - comment on the move, not the position
	- yes, image takes last N positions, but network is trained to evaluate the position, not the delta

PUCT - medium confidence
- try prior flattening with high nodes again, despite linear
	- 3qkb1r/3n1ppp/R1Npp3/6P1/QPr2P2/2P5/7P/4R1K1 b k - 2 27 - need Rxc6, avoid Qc8
	- can also help more backprops go to value rather than value+prior at high node counts
- try blend in prior softening towards endgame
	- 8/2Q5/8/2k5/6K1/8/3r4/q7 b - - 38 113 - avoid Kd4 - work around overly sharp priors
- try scale FPU up over time, gives exhaustive coverage, can use checks/winning captures heuristic and prior to differentiate
	- try hard exhaustive term (e.g. 1 per child after N visits)
- try value stddev
- last all-out attempt at position #3
	- try last-value again but scaled down (constant or f(n))
	- try killer heuristic for mate-in-N only - good examples in Arasan21 position #3 underneath Rf6, have to prove that many moves are losing

PUCT - expensive/failed ideas
- catch up on unpropagated value when returning via AZPUCT (+upSum, +upWeight)
- regret-pruning (+upSum, +upWeight)
- RAVE term, decaying with exploration (+amafSum, +amafWeight)

PUCT - finishing
- ablate PUCT terms, trim down to just parallelism/mate improvements, confidently measure what helps
- need long long search time parameter optimization for 1.25+ constant, e.g. applying similar to new terms

PUCT - later
- try use saturation point of visits/ucb to extend 800 simulations or UCI time control if visits would exceed current best
	- could this also be used earlier during search, backpropagate up a pending/bounty value via separate field to be replaced
	  by actual backprops - set it up when saturation point would take from below 1st to 1st in visits
- add move diversity temperature to mini-tournament optimization (may need lots of data, maybe soft-finish other params first)

Prediction cache (after PUCT work finished)
- investigate collisions (non-transposition style) during single search UCI affecting value convergence (details in 1/28 notes)
- test for average strength gain/loss ignoring path dependence and using in UCI/strengthtest for transpositions
	- no gain/loss earlier, try again after more PUCT work that does better for more nodes
- test speed gain for self-play generation ignoring path dependence, transposition table-style
- try stockfish-style prefetch asap after hash key update
- any more cache tuning? - hits about 70% full 40% hit rate after 3142 games, 12 ply max
	- while not full hit rate should be higher? add some new metrics
	- can probably remove max-ply if switching to transposition style - clear probe metrics after ramp-up, then measure

UCI
- manually dig into/debug STS strength drop with higher parallelism (1*16 vs. 2*256)
	- maybe enforce a threshold from 1st parallel PUCT to Nth after virtual losses - if too much lower, don't backprop
- already forcing value() for proved mate/opponent mate, but investigate positions and see whether mate term still helps M3 vs. M2, etc.
	- read over notes from initial work there, check for pitfalls
	- try no exploration terms for opponent-mate (test position: 6k1/4pppp/3B4/8/8/8/5PPP/R5K1 b - - 0 0)
	- try to make mate-visiting stop wasting time, instantly push up until an ancestor down to 2nd best
		- complicated with multiple levels of visits
		- complicated if 2nd best is also mate/terminal
		- only count as UCI "node" when expanding, not when terminal - same with initial expansions? - same with simulation count (800)?
- (still needed?) try out a backtracking implementation of virtual exploration/loss that goes as deep
	  as it can into good lines and can choose between 2nd best here vs. 2nd best up one level, etc.,
	  choosing better nodes overall and avoiding failed simulations
- (does this make any sense?) try speculative budgeting in addition to virtual exploration/loss
	- for N+small delta, which would have UCB > current best
	- can we still speculatively backprop, and is it needed after introducing scaled virtual loss
- profile CPU again
	- Game copy/assignment takes 20% CPU on _previousPositions copy, see if there's anything smarter
	- try vectorize/GPU softmax/UCT calcs
- tail Backpropagate into BackpropagateVisitsOnly
- basic improvements to time control
	- badly need to play well with X moves in Y, "movestogo", loses games vs. stockfish
	- don't waste time when only 1 or few legal moves
	- if dangerous time left, take mate-in-N rather than trying to find faster
- mate
	- if root node's terminal value is known, and allowed to stop searching, make a move instantly and save time
	- add global depth cut-off (a little tricky, need to allow MCTS to "back up" and ban paths that don't reach a leaf in time)
	- add killer heuristic (needs to vanish quickly with exploration)
		- test with M1 position with M1 prior artifically set low for ply 0
		- also strength-test
	- forced draw propagation if it'll help
- look into LCB move selection (Wu, LZ)
- implement worthwhile options (work out hash, TT vs. prediction cache)
- workaround for .exe vs. non in ChessBase, test as kibitzer
- implement proper time controls/search options, ponder

GUI
- change the square background rather than adding overlay, need more consistency between black/white squares

Code quality
- modify config.toml directly in sub-dockerfiles, don't rebuild/reinstall
- copy config.toml from install->user on run (if not present) - optimized for dev iteration atm
- switch to vcpkg for windows dependencies
- fix _ in network names - split on final _
- factor out all-exist ringbuffer with foreach iteration (begin/end)
- privatize C++ methods (got too python-brained)
- add Python type info everywhere - look up best practices for linting etc.
- check for any unnecessary move constructors
- StrengthTestNetwork (naming)

Deployment
- re-test everything on local/GPU and add comments for appropriate config for both (or implement HW-dependent config)
- installers, instructions, progress indicators, error handling, etc.
- linux unit test script
- add windows command line build script, binplace somewhere cleanly

Publishing
- write-ups
- final network weights, optimization run (30s move tournament mode)
- website - game-in-10, one at a time, people can watch, comments shown for each move
	
Bugs
- "quit" without "stop" hangs UCI
- "quit" after multiple "go" commands can hang UCI
- Some kind of race condition spamming next-chunk button in GUI, locks up Python/C++ interop
- Close cleanly if selfplay/UCI network not found by name in config list, in Python or C++
- SelfPlayWorker objects are created on the main thread and use its StateInfo in Game::Game(), then the worker thread steals in Free
- Rare ephemeral storage issue on cluster, might be core dumps

Low priority
- shouldn't use prediction cache with uniform predictions (CPU) - ugly to plumb

Won't do
- TensorRT or TF C++ API (overly complicated setup, especiallly building TF on Windows, embarrassingly bad)
- Mixed precision (depends on specific hardware, complicates code)
- Richer UCI features like multiPV or ponder
- endgame tablebases (e.g. leveraging Stockfish probing code)
	- UCI: not interesting enough, others can add if going for pure strength
	- Training: complicated, scope too large - self-play needs to make mistakes to learn imbalances
- contempt (self-play or UCI)
- ~100MB-sized chunks
	- complicated by supervised vs. self-play size, and time between networks for small self-play clusters
	- BigTable is an alternative if training needs to scale up to pods
- playout cap randomization (Wu)
	- great concept but greatly complicates training data compression, code, prefer simplicity in this trade-off
- finish-fast utility score added to win score a la SAI/Wu/lc0
	- useful to help with dithering, especially late-game, but not meeting the bar
- alpha manager improvements
	- allow for some roles to be broken/unassigned rather than bailing out
	- prioritize trainer over players, cannibalize
	- try delays rather than broken marker for tpus in case of temporary failures, throttling
- cyclic learning rate and/or fast-SWA (too much complexity too late)
- parameter optimization
	- categorical parameters
- performance
	- TF profiling - just live with what we have, v3-8 alpha TPUs decent as-is, not worth investment until more stable tooling, not using C++ inference anyway
	- use hard-elimination to deallocate sub-trees while searching
	- validation log still 11-12s after training log on cluster, try prefetch harder or debug
- training improvements (too expensive with remaining time/resources)
	- pseudonegatives (look up the paper again, may be difficult/impossible for chess, invalid positions)
	- investigate FROST - need to read more, but applied to data efficiency, seems like we could go in a couple directions:
		- use as intended, grab a bunch of unlabeled games, gradually pseudo-label better (end up with more data)
		- treat existing labels as pseudo-labels (e.g. poor-confidence value-from-final-result) and refine (end up with improved label quality)
	- try knowledge distillation for regression ideas: http://www.cs.ox.ac.uk/files/11078/ICCV19_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network.pdf
- GUI (analysis workflow is good enough inverted)
	- invert uci/gui relationship after working out in-proc/out details
		- switch between database/UCI modes
		- browse EPDs
		- set up positions (paste pgn, fen, move pieces, uci from database position)
		- go X buttons
	- deal with info panel height/scrolling, move list
	- websocket reconnect
- PUCT ideas
	- many too expensive, difficult, volatile, etc.