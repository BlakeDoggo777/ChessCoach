Tests
- add unit test for unique queen/knight indexing into planes
- add test for null move flipping the generated image (important for commentary data)

Self-play
- *** CPU/mcts times going up over time, again, important to check for node/stateinfo leaks ***
	- need to be able to let TPU machines run
- move uniform to pre-PythonNetwork C++
- switch to multiple MCTS threads per python thread (2:1 should be fine if prediction cache on)

Commentary
- not converging after restoration, sync back and check

Check in
- make sure main.py all working on GPU + TPU
- make sure all exe tools working, debug/release

Apply for cloud resources
- look up the FAQ again

Validate
- make sure that a fresh run can match chesscoach6 performance by 15k steps

AFTER CHECK-IN, FIND A HOME LATER
- print config summary when starting a run
- move to ~100MB-sized chunks as recommended (but make sure self-play can still viably train)
- don't need to copy vectors into SavedGame, see if the struct is still useful at all, maybe just richer callbacks
- try also batching across examples in dataset.py
- need to solve pipeline threading issues again, make sure Cuda >= 90% locally
	- also "experimental_steps_per_execution" for TPU
- throw away positions before expensive decompression in dataset.py, keep all/more games, keep fewer positions
- relinquish prediction cache memory while training? increase cycle/shuffle?
- anything that uses a worker.front() will create new poolallocators on the primary (non-worker) thread, higher memory ceiling
- save fewer networks to cloud storage or cull old ones in between strength tests or something - 100 MB per save
- overall run may hang or have 2k too few games if any .game files deleted now, think about live adjustment/different approach

Distributed self-play/training
- set up some basic infrastructure, preemptibles, startup, roles, etc.
	- don't lose 1999 games when preemptible goes down

Training improvements
- playout cap randomization (Wu)
- forced playouts/pruning (Wu)
- SE (lc0) or SE-ish-as-bias (Wu)
- pseudonegatives (look up the paper again, may be difficult/impossible for chess, invalid positions)
- try 1 vs. 8 vs. 32 value head filters again
	- lc0/Wu seem to use similar conv filters for policy and value heads?
	- lc0 was thinking fully connected layer may be needed for policy head when network isn't deep enough to cross board well (may not apply for distillation)
- try a run with masked policy loss (valid moves only) once there's enough data to compare

Value experiments
- sketch out "value trust curve", especially with volatile game, multiple mistakes both sides
- work out static eval vs. search on stockfish side for inflection/control points, see what's viable

Commentary cleanup
- use model garden transformer now?
- try wrap main engine loop with @tf.function too?
- wrap <start>/<end> in graph mode - Transformer article
- cull attention weight plumbing
- can pre-sequence-and-index commentary
- write unit tests for comment preprocessing
- try time and special-case speed-up prediction for batchsize=1

Text quality
- basic - does the encoder need an extra dense+relu layer after the tower/reshape?
- tokenizing - lots of options for chess specifically, around punctuation, top-k, etc.
- larger model size
- different model architectures
	- read about "Performer" architecture
	- read about PET: https://arxiv.org/pdf/2001.07676.pdf
	- definitely try universal transformer
- different tokenizers
	- definitely try SubwordTextEncoder
- build deterministic train/validate split?
- try encode before + after positions - comment on the move, not the position
	- yes, image takes last N positions, but network is trained to evaluate the position, not the delta

Additional head ideas:
- (Wu): "whenever a desired target can be expressed as a sum, conjunction, or disjunction of separate subevents, or would be highly correlated with such subevents, predicting those subevents is likely to help""
- autoencode position
- count all pieces?
- count total square control/ratio/difference/"policy"?
- count piece types
- count piece imbalances
- predict opponent move (Wu uses 0.15 factor)
- principle variation (big output surface, complicated idea)
- predict final board state or piece distribution (like Wu's final ownership)
- king safety, mobility, etc.

At 20-30k steps:
- test without mcts value/reply policy, see whether they truly help/hurt
- try out "sleeping" in the rotation - train random noise, different distributions
- make sure piece-value understanding is improving; think about standard A/B positions to log avg queen/rook/etc. value to tensorboard

Once value loss is converging below 0.4 on big network:
- Try knowledge distillation for regression ideas: http://www.cs.ox.ac.uk/files/11078/ICCV19_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network.pdf

Training - later
- like Wu/lc0, think about finish-fast utility score added to win score - way later though, only useful if it plays well but dithers
- stochastic weight averaging

TPUs - later
- try make the student a bit deeper - already hugely smaller with 64 vs. 256 filters (benchmark self-play and UCI, GPU and TPU, not much headroom on GTX 1080 though)
- try C++ API if we still need more games, could do it linux-only
	(maybe https://github.com/FloopCZ/tensorflow_cc)
	(Q4? https://github.com/tensorflow/tensorflow/issues/42614)
	(include file tip https://stackoverflow.com/questions/42898577/list-of-headers-to-use-tensorflow-c-api-using-libtensorflow-cc-so)

UCI (#2)
- tune on GPU and TPU (methods below)
- try to increase in-thread parallelism without sacrificing STS strength
	- some kind of idea around only the first N backpropagating, the rest just setting up children/priors?
	- can always look into some of the failing positions and see what's going differently (harder without multiPV though)
- also test different FPU schemes for strength, but care, and def. don't change training
- try multi-thread
	- need to sync std::cout - use Reply() everywhere
- basic improvements to time control
	- don't waste time when only 1 or few legal moves
	- if dangerous time left, take mate-in-N rather than trying to find faster

CPU+GPU integration performance:
- maybe deparallelize SelfPlayWorker arrays
- try shrink Node memory
- try vectorize/GPU softmax/UCT calcs
- break down MCTS timing details just in case anything's off (just use sampling)
- image key for prediction cache can be incrementally updated if it helps enough
- do some timeline charting for CPU vs. GPU use
	- it may make sense to keep shared images/values/policies but have separate CPU worker threads
	  drop in/pick up there, using a GPU worker like very beginning, and maybe oversubscribe
	  like double-buffer games? might just be equivalent to doubling threads though. seem to be
	  limited by Node memory use now. anyway, think about it.
- buffer fstreams for windows
- for Nodes can point last sibling at root and unrecurse deletion (sacrificing breadth-at-once)

Code quality
- copy config.toml from install->user on run (if not present) - optimized for dev iteration atm
- switch to vcpkg for windows dependencies
- fix _ in network names - split on final _
- factor out all-exist ringbuffer with foreach iteration (begin/end)
- privatize C++ methods (got too python-brained)
- try split up MCTS vs. selfplaygames better - e.g. maybe move softmax/selectmove out? separate files?
- try split SelfPlayGame/Worker vs. "UciGame/Worker" better somehow (including image/value/policy arrays, batch sizes, etc.)
- improve config use in C++, feels messy
- add Python type info everywhere - look up best practices for linting etc.
- poison self-play workers and join threads properly
- check for any unnecessary move constructors
- add further architecture configuration to toml config (residuals*filters, etc.)
- switch to sample ratio, rather than specifying games + steps (complicated, ties in to "curriculum schedule" in config maybe)
- StrengthTestNetwork (naming)

UCI (#3)
- "random move with eval within 1%"
	- if best is value N, softmax sample with temperature 10 among (value >= N - 0.01), where 0.01 = (win-loss)/100
	- "best" I'm assuming is the otherwise usual selection, i.e. most visited
	- code for shorter mates, etc., should override this behavior (no diversity needed then)
- mate
	- if root node's terminal value is known, and allowed to stop searching, make a move instantly and save time
	- add global depth cut-off (a little tricky, need to allow MCTS to "back up" and ban paths that don't reach a leaf in time)
	- add killer heuristic (needs to vanish quickly with exploration)
		- test with M1 position with M1 prior artifically set low for ply 0
		- also strength-test
	- forced draw propagation if it'll help
- look into LCB move selection (Wu, LZ)
- implement worthwhile options (work out hash, TT vs. prediction cache)
- workaround for .exe vs. non in ChessBase, test as kibitzer
- implement proper time controls/search options, ponder

Deployment
- installers, instructions, progress indicators, error handling, etc.
- linux unit test script

Publishing
- write-ups
- website - game-in-10, one at a time, people can watch, comments shown for each move

Low priority
- structured binding, check all pairs

Revisit
- try batch renorm + ghost batch norm
	- Bug: https://github.com/tensorflow/tensorflow/issues/32380
- any more cache tuning? - hits about 70% full 40% hit rate after 3142 games, 12 ply max
	- while not full hit rate should be higher? add some new metrics

Bugs
- Close cleanly if selfplay/UCI network not found by name in config list, in Python or C++
- Still may be a leak between multiple self-play rotations separated by training - doesn't require strength testing to repro
- Possible problems resuming if e.g. student missing or strength tests didn't run, etc. - no per-stage completion checks

Won't do
- TensorRT or TF C++ API (overly complicated setup, especiallly building TF on Windows, embarrassingly bad)
- Richer UCI features like multiPV
- GUI for play/debug, only UCI debug commands
- CPU optimization - not measurable enough while GPU-dominated
- endgame tablebases (e.g. leveraging Stockfish probing code)
	- UCI: not interesting enough, others can add if going for pure strength
	- Training: complicated, scope too large - self-play needs to make mistakes to learn imbalances