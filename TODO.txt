Check UCI/search MCTS details
- does mctsSimulation in RunMcts ever overflow for TryHard()?
- do we ever call SelectMove() after 1k loops for TryHard()?
- once a node fails, are the rest guaranteed to follow the same path and fail on the same thread? short-circuit?

Training
- try fpu-weight instead of forced playouts - use weight = 1 or 2 instead of 0 for root children during self-play
	- find some easy sacrifice + win positions, make sure it gets multiple visits instead of 1 and stopping with various noise
		-  rn3rk1/5ppp/p3pn2/1bpq4/8/2PB3P/5PP1/QN1R2K1 w - - 0 20
	- still need to prune though or no?
- add back forced playouts + pruning? or add 8x virtual virtual loss during self-play?
- after plateauing - try teacher-only prediction w/ SWA again (conditional config if it works)
- full run - 1/N positions using K nodes (e.g. 1/20, 50k) - acts like auxiliary head at low loss weight, points good direction
	- need to emulate parallelism/vloss/wideband for the 50k, or 1*1 okay?
- full run - 15% chance to syzygy probe root or WDL deeper during self-play - acts like auxiliary head at low loss weight, points good direction
- full run - add repetition plane per history position to NN inputs (0 = 1-rep, 1=2rep)

Commentary #3
- need fix for tensorflow_text on alpha TPU VMs - lazy-import for now
- remove dropout for now - can slowly add back decoder-only - may need to avoid it for highly discrete board squares in the encoder sequence
- investigate inference problems locally (initial code is stashed)
- build small manual test set for commentary inference
- may need to increase maximum sequence length, with sub-word tokenization
- maybe backprop into encoder (sometimes? always?)
- compare interleaving with main model training vs. "at the end"
- compare adam and sgd-momentum for commentary
	- try varying warmup steps
	- try flip to pre-LN, get rid of warmup everywhere: https://openreview.net/pdf?id=B1x8anVFPr
- review/tune commentary quality
- compare beam and top-k
	- look back through my notes, was pretty pessimistic about beam based on articles a few months back

Commentary #4
- try pre or post-processing with GPT-2
- try data augmentation using BERT: https://www.researchgate.net/publication/343958685_Text_Augmentation_Using_BERT_for_Image_Captioning
- maybe shrink model if still not enough data

Commentary #5
- for interleaving regular/commentary training - read up on "learning without forgetting"
- basic - does the encoder need an extra dense+relu layer after the tower/reshape?
	- basically 256->512 embedding rather than categorical->512 embedding
- try encode before-and-after positions (no flip?) - comment on the move, not the position
	- yes, image takes last N positions, but network is trained to evaluate the position, not the delta

PUCT - medium confidence
- try prior flattening/absolute linear (still respect value) with high nodes again, despite linear
	- 3qkb1r/3n1ppp/R1Npp3/6P1/QPr2P2/2P5/7P/4R1K1 b k - 2 27 - need Rxc6, avoid Qc8
	- kr6/ppq3b1/2pNQ1p1/7p/7P/1R4P1/P4PB1/3n2K1 w - - 0 32 - find Rxb7 M10 - gets stuck on Ne8
	- can also help more backprops go to value rather than value+prior at high node counts
- try blend in prior softening towards endgame
	- 8/2Q5/8/2k5/6K1/8/3r4/q7 b - - 38 113 - avoid Kd4 - work around overly sharp priors
- try scale FPU up over time, gives exhaustive coverage, can use checks/winning captures heuristic and prior to differentiate
	- try hard exhaustive term (e.g. 1 per child after N visits - maybe M grandparent visits)
	- 8/8/8/2P3R1/5B2/2rP1p2/p1P1PP2/RnQ1K2k w Q - 5 3 - find M2
- try value stddev
- last all-out attempt at position #3
	- try last-value again but scaled down (constant or f(n))
	- try killer heuristic for mate-in-N only - good examples in Arasan21 position #3 underneath Rf6, have to prove that many moves are losing - decay
- think of ways to catch up visits when suddenly finding value at very high nodes
	- some kind of high-trust/verification PUCT saturation calc?
	- backpropagate more than 1 weight/visit per, for high overall visits + deep leaf?
- work on making more progress in endgames - may be worse now with higher NPS from cache-as-TT
	- 8/8/8/4ppk1/8/8/6K1/8 b - - 0 84

PUCT - finishing
- ablate PUCT terms, trim down to just parallelism/mate improvements, confidently measure what helps
- need long long search time parameter optimization for 1.25+ constant, e.g. applying similar to new terms
- try UCI move diversity again, maybe visit% threshold param, maybe first N moves param, optimize

PUCT - later
- try use saturation point of visits/ucb to extend 800 simulations or UCI time control if visits would exceed current best
	- could this also be used earlier during search, backpropagate up a pending/bounty value via separate field to be replaced
	  by actual backprops - set it up when saturation point would take from below 1st to 1st in visits
- add move diversity temperature to mini-tournament optimization (may need lots of data, maybe soft-finish other params first)

UCI
- manually dig into/debug STS strength drop with higher parallelism (1*16 vs. 2*256)
	- maybe enforce a threshold from 1st parallel PUCT to Nth after virtual losses - if too much lower, don't backprop
- tail Backpropagate into BackpropagateVisitsOnly
- basic improvements to time control
	- don't waste time when only 1 legal move
	- if dangerous time left, take mate-in-N or TB win rather than trying to find faster
- workaround for .exe vs. non in ChessBase - can we probe for virtuals and activate before initializing python?
- implement remaining commands/options - searchmoves - spin option min/max - check spec again
- set prediction cache size (hash size) by MiB - more consistent with other engines, already mostly supported
- cache-as-TT - do we add repetition state (1-rep vs. 2-rep) and/or no-progress count for current position (bucketed?)?

GUI
- don't error on supervised data - use prior, no need to match result (often adjudicated)
- change the square background rather than adding overlay, need more consistency between black/white squares

Bugs
- Some kind of race condition spamming next-chunk button in GUI, locks up Python/C++ interop
- Close cleanly if selfplay/UCI network not found by name in config list, in Python or C++
- SelfPlayWorker objects are created on the main thread and use its StateInfo in Game::Game(), then the worker thread steals in Free
- shouldn't use prediction cache with uniform predictions (CPU) - ugly to plumb

Code quality
- instead of Optimizer.iterations, make subset/full share optimizer, save/load optimizer weights, validate .iterations before training
- modify config.toml directly in sub-dockerfiles, don't rebuild/reinstall
- copy config.toml from install->user on run (if not present) - optimized for dev iteration atm
- switch to vcpkg for windows dependencies
- fix _ in network names - split on final _
- privatize C++ methods (got too python-brained)
- add Python type info everywhere - look up best practices for linting etc.
- check for any unnecessary move constructors
- StrengthTestNetwork (naming)
- tail common code in ExpandAndEvaluate? (goto?)

Deployment
- re-test everything on local/GPU and add comments for appropriate config for both (or implement HW-dependent config)
	- student on GPU, 4*256, teacher/student(which?) on TPU, 16*256 for both
	- make sure multi-GPU-on-desktop is supported - reevaluate params
- installers, instructions, progress indicators, error handling, etc.
- linux unit test script
- add windows command line build script, binplace somewhere cleanly

Publishing
- write-ups
- final network weights, optimization run (30s move tournament mode)
- website - game-in-10, one at a time, people can watch, comments shown for each move

Won't do
- TensorRT or TF C++ API (overly complicated setup, especiallly building TF on Windows, embarrassingly bad)
- Mixed precision (depends on specific hardware, complicates code)
- Richer UCI features like multiPV or ponder
- contempt (self-play or UCI)
- ~100MB-sized chunks
	- complicated by supervised vs. self-play size, and time between networks for small self-play clusters
	- BigTable is an alternative if training needs to scale up to pods
- playout cap randomization (Wu)
	- great concept but greatly complicates training data compression, code, prefer simplicity in this trade-off
- finish-fast utility score added to win score a la SAI/Wu/lc0
	- useful to help with dithering, especially late-game, but not meeting the bar
- alpha manager improvements
	- allow for some roles to be broken/unassigned rather than bailing out
	- prioritize trainer over players, cannibalize
	- try delays rather than broken marker for tpus in case of temporary failures, throttling
- cyclic learning rate and/or fast-SWA (too much complexity too late)
- parameter optimization
	- categorical parameters
- performance
	- TF profiling - just live with what we have, v3-8 alpha TPUs decent as-is, not worth investment until more stable tooling, not using C++ inference anyway
	- use hard-elimination to deallocate sub-trees while searching
	- validation log still 11-12s after training log on cluster, try prefetch harder or debug
- training improvements (too expensive with remaining time/resources)
	- pseudonegatives (look up the paper again, may be difficult/impossible for chess, invalid positions)
	- investigate FROST - need to read more, but applied to data efficiency, seems like we could go in a couple directions:
		- use as intended, grab a bunch of unlabeled games, gradually pseudo-label better (end up with more data)
		- treat existing labels as pseudo-labels (e.g. poor-confidence value-from-final-result) and refine (end up with improved label quality)
	- try knowledge distillation for regression ideas: http://www.cs.ox.ac.uk/files/11078/ICCV19_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network.pdf
- GUI (analysis workflow is good enough inverted)
	- invert uci/gui relationship after working out in-proc/out details
		- switch between database/UCI modes
		- browse EPDs
		- set up positions (paste pgn, fen, move pieces, uci from database position)
		- go X buttons
	- deal with info panel height/scrolling, move list
	- websocket reconnect
- PUCT ideas
	- many too expensive, difficult, volatile, etc.